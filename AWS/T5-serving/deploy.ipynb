{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f25086-b2fa-4e7e-947d-6273b7dbb24c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.209.0\n"
     ]
    }
   ],
   "source": [
    "import sys, sagemaker, boto3\n",
    "!{sys.executable} -m pip install -qU \"sagemaker>=2.11.0\"\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ed9f2a-cd86-4f02-9657-92d35821441f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "4.38.2\n",
      "Python 3.10.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e62b2f-1f75-4928-85bf-a2667aa4a773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "sess = Session()\n",
    "role = get_execution_role()\n",
    "print(f'Current session is : {sess}, \\nCurrent role is : {role}')\n",
    "\n",
    "# check default bucket\n",
    "sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4caae88-0c16-4b00-9081-bc0bca6e1b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.list_buckets()\n",
    "\n",
    "# 버킷 이름 출력\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffba16c-63d3-4026-9bd8-2492d08895cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = \"error-correction\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e93be-9d14-4c42-897f-f82e27f70441",
   "metadata": {
    "tags": []
   },
   "source": [
    "## S3 to SageMakerNotebook\n",
    "    - S3에 디아카이빙된 파일들을 올린 상태\n",
    "    - 다운로드해온 후 해당 노트북 인스턴스에서 inference.py(handler.py) 코드 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2d3cfe5-f43d-4d6a-a02d-8422f01a85fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan_t5_xlarge/config.json\n",
      "flan_t5_xlarge/err_corr_generation.ipynb\n",
      "flan_t5_xlarge/generation_config.json\n",
      "flan_t5_xlarge/handler.py\n",
      "flan_t5_xlarge/postprocess.py\n",
      "flan_t5_xlarge/pytorch_model.bin\n",
      "flan_t5_xlarge/special_tokens_map.json\n",
      "flan_t5_xlarge/tokenizer.json\n",
      "flan_t5_xlarge/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# S3 클라이언트 초기화\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 버킷 및 폴더 이름 지정\n",
    "bucket = \"error-correction\"\n",
    "prefix = 'flan_t5_xlarge/'  # 폴더 이름\n",
    "\n",
    "# 지정된 폴더 내의 모든 파일 나열\n",
    "def list_files_in_folder(bucket, prefix):\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    files = []\n",
    "\n",
    "    # 폴더 내의 모든 파일을 나열\n",
    "    for item in response.get('Contents', []):\n",
    "        files.append(item['Key'])\n",
    "\n",
    "    return files\n",
    "\n",
    "files = list_files_in_folder(bucket, prefix)\n",
    "files = files[1:] # 첫번째 의미없는 폴더 제거\n",
    "\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7227316-517d-4b84-b3c2-d950650719de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded flan_t5_xlarge/config.json to flan_t5_xlarge/config.json\n",
      "Downloaded flan_t5_xlarge/err_corr_generation.ipynb to flan_t5_xlarge/err_corr_generation.ipynb\n",
      "Downloaded flan_t5_xlarge/generation_config.json to flan_t5_xlarge/generation_config.json\n",
      "Downloaded flan_t5_xlarge/handler.py to flan_t5_xlarge/handler.py\n",
      "Downloaded flan_t5_xlarge/postprocess.py to flan_t5_xlarge/postprocess.py\n",
      "Downloaded flan_t5_xlarge/pytorch_model.bin to flan_t5_xlarge/pytorch_model.bin\n",
      "Downloaded flan_t5_xlarge/special_tokens_map.json to flan_t5_xlarge/special_tokens_map.json\n",
      "Downloaded flan_t5_xlarge/tokenizer.json to flan_t5_xlarge/tokenizer.json\n",
      "Downloaded flan_t5_xlarge/tokenizer_config.json to flan_t5_xlarge/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 파일 다운로드 함수\n",
    "def download_file(bucket, s3_file_path, local_save_path):\n",
    "    # 필요한 경우 로컬 저장 경로의 디렉토리 생성\n",
    "    os.makedirs(os.path.dirname(local_save_path), exist_ok=True)\n",
    "    \n",
    "    # 파일 다운로드\n",
    "    s3.download_file(bucket, s3_file_path, local_save_path)\n",
    "    print(f'Downloaded {s3_file_path} to {local_save_path}')\n",
    "\n",
    "# 로컬 저장 경로 지정 (예시)\n",
    "local_folder = 'flan_t5_xlarge'\n",
    "\n",
    "# 파일 다운로드 실행\n",
    "for file_key in files:\n",
    "    local_file_path = os.path.join(local_folder, os.path.basename(file_key))\n",
    "    download_file(bucket, file_key, local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba94615-c78d-4732-82d6-877281cd8bc4",
   "metadata": {},
   "source": [
    "## inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b80814c-8ae7-4bb4-b70b-e7afcf072a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flan_t5_xl.inference import model_fn, input_fn, predict_fn, output_fn\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb740c4-36f8-476e-b3e4-1b0470b778ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# model 저장 경로\n",
    "model_dir = 'flan_t5_xl'\n",
    "\n",
    "model_handler = model_fn(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b956bd-b5d1-4ab6-8b84-3f18ecbef985",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resquest_body :  [{\"data\": \"Tutor: Thanks, love. But let's focus on planning our anniversary first. Any suggestions? Student: you very nice Tutor: Aww, you're sweet. Now, let's talk about our anniversary dinner. Do you have any cuisine in mind? [Improve]Student: How about a picnic?\"}, {\"data\": \"Tutor: Sure, how about this navy blue suit with a thin light-blue stripe? [Improve]Student: This stripe is too small. Is there anything bigger?\"}]\n",
      "input_data [{'data': \"Tutor: Thanks, love. But let's focus on planning our anniversary first. Any suggestions? Student: you very nice Tutor: Aww, you're sweet. Now, let's talk about our anniversary dinner. Do you have any cuisine in mind? [Improve]Student: How about a picnic?\"}, {'data': 'Tutor: Sure, how about this navy blue suit with a thin light-blue stripe? [Improve]Student: This stripe is too small. Is there anything bigger?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utterances :  ['How about a picnic?', 'This stripe is too small. Is there anything bigger?']\n",
      "utter :  how about a picnic?\n",
      "corr :  how about a picnic?\n",
      "utter :  this stripe is too small is there anything bigger?\n",
      "corr :  this stripe is too small is there anything bigger?\n",
      "{\"generated_text\": [\"how about a picnic?\", \"this stripe is too small is there anything bigger?\"]}\n"
     ]
    }
   ],
   "source": [
    "# 추론을 위한 입력 데이터를 준비합니다.\n",
    "dialogue = [\n",
    "    \"Tutor: Thanks, love. But let's focus on planning our anniversary first. Any suggestions? Student: you very nice Tutor: Aww, you're sweet. Now, let's talk about our anniversary dinner. Do you have any cuisine in mind? [Improve]Student: How about a picnic?\",\n",
    "    \"Tutor: Sure, how about this navy blue suit with a thin light-blue stripe? [Improve]Student: This stripe is too small. Is there anything bigger?\"\n",
    "]\n",
    "\n",
    "# dialogue를 각 항목별로 JSON 형식으로 변환합니다. 이 예시에서는 각 대화를 'data' 키를 가진 딕셔너리로 만듭니다.\n",
    "request_body = json.dumps([{\"data\": text} for text in dialogue])\n",
    "print('resquest_body : ', request_body)\n",
    "\n",
    "request_content_type = 'application/json'\n",
    "\n",
    "# 입력 데이터를 전처리합니다.\n",
    "input_data = input_fn(request_body, request_content_type)\n",
    "# print('input data : ', input_data)\n",
    "\n",
    "# 모델 추론을 실행합니다.\n",
    "prediction = predict_fn(input_data, model_handler)\n",
    "# print('prediction : ', prediction)\n",
    "\n",
    "# 추론 결과를 처리합니다.\n",
    "output = output_fn(prediction, 'application/json')\n",
    "\n",
    "# 추론 결과를 출력합니다.\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c806bf-b89c-4d5d-b92d-21b5b7775fbd",
   "metadata": {},
   "source": [
    "## model.tar.gz 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c01745-57de-4c0c-9ffd-fef337dbc96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar -czvf model.tar.gz config.json generation_config.json tokenizer.json tokenizer_config.json special_tokens_map.json pytorch_model.bin inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67366911-d8e0-4f6c-9550-2d5a5068c1a6",
   "metadata": {},
   "source": [
    "## s3에 model.tar.gz 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e206a3-3b39-4214-b155-3517875d5b13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading model.tar.gz...\n",
      "Uploaded model.tar.gz to flan_t5_xlarge/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# AWS 접속 정보\n",
    "ACCESS_KEY_ID = \n",
    "ACCESS_SECRET_KEY = \n",
    "BUCKET_NAME = 'error-correction'\n",
    "\n",
    "# 업로드할 파일이 위치한 폴더\n",
    "FOLDER_PATH = 'flan_t5_xl'\n",
    "\n",
    "# 업로드할 파일 리스트\n",
    "upload_files = ['model.tar.gz']\n",
    "\n",
    "def upload_files_to_s3(folder_path, upload_files):\n",
    "    # S3 클라이언트 생성\n",
    "    s3_client = boto3.client('s3',\n",
    "                             aws_access_key_id=ACCESS_KEY_ID,\n",
    "                             aws_secret_access_key=ACCESS_SECRET_KEY)\n",
    "\n",
    "    # 지정된 파일만 업로드\n",
    "    for filename in upload_files:\n",
    "        local_file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.exists(local_file_path):  # 파일 존재 여부 확인\n",
    "            print(f\"Uploading {filename}...\")\n",
    "            s3_target_path = 'flan_t5_xlarge/' + filename  # S3 내의 대상 경로 설정\n",
    "\n",
    "            # 파일 업로드\n",
    "            s3_client.upload_file(local_file_path, BUCKET_NAME, s3_target_path)\n",
    "            print(f'Uploaded {filename} to {s3_target_path}')\n",
    "        else:\n",
    "            print(f\"{filename} does not exist in {folder_path}\")\n",
    "\n",
    "# 함수 호출\n",
    "upload_files_to_s3(FOLDER_PATH, upload_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea146251-03c8-4236-a46f-32405af74398",
   "metadata": {},
   "source": [
    "## SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702eeaca-a431-45dd-99ef-bf7f01dce9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "sess = Session()\n",
    "role = get_execution_role()\n",
    "print(f'Current session is : {sess}, \\nCurrent role is : {role}')\n",
    "\n",
    "# check default bucket\n",
    "sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce696e6-71b2-4be2-9881-7df594ab7770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.pytorch.model import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090b18e7-b9a0-4ed5-b977-595d6d46430d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 697 µs, sys: 0 ns, total: 697 µs\n",
      "Wall time: 494 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "model_path = \"s3://error-correction/flan_t5_xlarge/model.tar.gz\"\n",
    "endpoint_name = \"endpoint-pytorch-{}\".format(int(time.time()))\n",
    "\n",
    "model = PyTorchModel(model_data=model_path,\n",
    "                     role=role,\n",
    "                     entry_point='inference.py', \n",
    "                     source_dir='src',\n",
    "                     framework_version='2.0.1',\n",
    "                     py_version='py310')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0ab47-9030-468f-8280-086ebb1d39f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!tar -czvf model.tar.gz inference.py config.json generation_config.json tokenizer.json tokenizer_config.json special_tokens_map.json pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c42f9e1f-ca99-4ad6-a77b-7eaa4bf0153c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g4dn.2xlarge',\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "213cccce-0b32-47ce-b616-82afb82ed3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cce9362-2427-4b41-9a7c-dbe968ff0471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input = [{\n",
    "   \"data\": \"Tutor: Thanks, love. But let's focus on planning our anniversary first. Any suggestions? Student: you very nice Tutor: Aww, you're sweet. Now, let's talk about our anniversary dinner. Do you have any cuisine in mind? [Improve]Student: How about a picnic?\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63d73de4-62ee-48ce-a372-211597e289f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': ['how about a picnic?']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "233bae1e-b2fa-470e-a5a8-2871d8340aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API call duration: 0.26 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"generated_text\": [\"i had symptom\"]}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "content_type = \"application/json\"\n",
    "request_body = [{'data': \"i had symptom\"}]\n",
    "data = json.loads(json.dumps(request_body))\n",
    "payload = json.dumps(data)\n",
    "\n",
    "# API 호출 전 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# API 호출\n",
    "response = client.invoke_endpoint(\n",
    "EndpointName=endpoint_name,\n",
    "ContentType=content_type,\n",
    "Body=payload)\n",
    "\n",
    "# API 호출 후 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 호출 속도 계산\n",
    "duration = end_time - start_time\n",
    "\n",
    "# 출력\n",
    "print(f\"API call duration: {duration:.2f} seconds\")\n",
    "\n",
    "# 호출 결과\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb526d13-ad4f-4f2b-b375-19d8832b3cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request start time: 1709801305.74, Duration: 0.94 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.74, Duration: 0.52 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 1.14 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 0.31 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 1.76 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 0.72 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 1.55 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 3.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 1.97 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 2.59 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.75, Duration: 2.17 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.76, Duration: 2.38 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.77, Duration: 2.99 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.77, Duration: 2.78 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.78, Duration: 4.02 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.78, Duration: 4.22 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.78, Duration: 3.60 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.79, Duration: 1.31 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.79, Duration: 4.63 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.79, Duration: 4.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.80, Duration: 3.17 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.80, Duration: 5.04 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.80, Duration: 3.78 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.80, Duration: 4.82 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.81, Duration: 5.64 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.81, Duration: 5.85 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.81, Duration: 6.06 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.82, Duration: 5.22 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.82, Duration: 6.47 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.82, Duration: 6.26 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.83, Duration: 6.67 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.83, Duration: 6.88 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.83, Duration: 5.41 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.83, Duration: 7.91 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.84, Duration: 7.29 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.84, Duration: 7.07 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.85, Duration: 9.54 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.85, Duration: 8.31 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.85, Duration: 8.10 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.86, Duration: 9.33 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.86, Duration: 8.91 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.87, Duration: 8.70 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.87, Duration: 7.66 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.87, Duration: 9.73 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.88, Duration: 7.45 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.88, Duration: 9.10 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.88, Duration: 8.49 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.88, Duration: 10.13 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.88, Duration: 9.92 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.89, Duration: 10.54 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.90, Duration: 10.94 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.90, Duration: 11.97 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.91, Duration: 12.17 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.91, Duration: 11.14 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.91, Duration: 11.55 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.91, Duration: 13.19 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.91, Duration: 10.72 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.92, Duration: 10.31 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.92, Duration: 11.74 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.92, Duration: 12.36 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.92, Duration: 11.33 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.93, Duration: 12.56 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.93, Duration: 14.22 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.94, Duration: 12.75 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.94, Duration: 13.79 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.95, Duration: 13.99 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.95, Duration: 13.57 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.95, Duration: 14.83 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.96, Duration: 14.40 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.96, Duration: 12.94 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.96, Duration: 15.85 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.96, Duration: 14.60 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.97, Duration: 13.35 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.97, Duration: 15.64 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.97, Duration: 16.26 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.98, Duration: 15.22 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.98, Duration: 15.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.99, Duration: 16.86 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801305.99, Duration: 17.07 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.00, Duration: 16.65 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.00, Duration: 14.99 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.00, Duration: 17.27 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.00, Duration: 17.47 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.01, Duration: 18.09 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.01, Duration: 18.29 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.01, Duration: 16.43 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.02, Duration: 17.66 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.02, Duration: 17.87 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.03, Duration: 19.93 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.03, Duration: 18.48 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.04, Duration: 19.51 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.04, Duration: 19.30 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.04, Duration: 18.67 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.04, Duration: 20.33 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.05, Duration: 20.12 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.05, Duration: 19.08 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.05, Duration: 15.97 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.06, Duration: 18.86 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.06, Duration: 19.69 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709801306.07, Duration: 20.51 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Total duration for all requests: 20.84 seconds\n"
     ]
    }
   ],
   "source": [
    "# concurrent calls \n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# request function\n",
    "def send_request(request_data):\n",
    "    start = time.time()  \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,  \n",
    "        ContentType=content_type,\n",
    "        Body=json.dumps(request_data)\n",
    "    )\n",
    "    end = time.time()  \n",
    "    duration = end - start  \n",
    "    return start, duration, response['Body'].read().decode('utf-8')\n",
    "\n",
    "\n",
    "# 동시 요청의 수\n",
    "num_requests = 100\n",
    "\n",
    "\n",
    "# ThreadPoolExecutor를 사용하여 동시 요청 수행\n",
    "with ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "    futures = [executor.submit(send_request, data) for _ in range(num_requests)]\n",
    "\n",
    "    # 결과 출력\n",
    "    for future in futures:\n",
    "        start_time, duration, result = future.result()\n",
    "        print(f\"Request start time: {start_time:.2f}, Duration: {duration:.2f} seconds, Result: {result}\")\n",
    "\n",
    "# 전체 요청 완료 후의 현재 시간\n",
    "overall_end_time = time.time()\n",
    "\n",
    "# 시작 시간 중 가장 빠른 시간 찾기\n",
    "start_times = [future.result()[0] for future in futures]\n",
    "overall_start_time = min(start_times)\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "print(f\"Total duration for all requests: {overall_end_time - overall_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6b18e20-2a7d-40c3-aad0-d77312a49aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request start time: 1709803115.98, Duration: 1.54 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.98, Duration: 0.72 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 0.27 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 1.13 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 0.51 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 4.00 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 1.74 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 2.15 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 1.33 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 1.94 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803115.99, Duration: 2.97 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.00, Duration: 2.35 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.00, Duration: 2.76 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.00, Duration: 3.16 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.00, Duration: 6.45 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.02, Duration: 0.89 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.02, Duration: 2.53 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.02, Duration: 3.76 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.03, Duration: 4.38 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.03, Duration: 4.78 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.03, Duration: 5.60 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.04, Duration: 4.16 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.04, Duration: 4.57 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.05, Duration: 5.80 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.05, Duration: 3.32 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.06, Duration: 3.52 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.07, Duration: 5.36 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.07, Duration: 5.98 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.07, Duration: 6.79 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.07, Duration: 9.04 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.07, Duration: 6.17 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.08, Duration: 5.15 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.08, Duration: 6.57 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.09, Duration: 4.93 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.09, Duration: 8.00 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.09, Duration: 6.97 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.09, Duration: 8.21 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.09, Duration: 8.61 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.10, Duration: 7.17 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.10, Duration: 7.57 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.11, Duration: 9.83 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.11, Duration: 7.36 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.11, Duration: 8.39 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.11, Duration: 7.77 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.12, Duration: 9.20 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.12, Duration: 9.40 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.13, Duration: 8.79 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.13, Duration: 10.62 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.14, Duration: 9.59 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.14, Duration: 10.41 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.14, Duration: 11.02 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.15, Duration: 9.99 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.15, Duration: 11.22 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.15, Duration: 10.19 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.15, Duration: 12.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.16, Duration: 11.61 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.16, Duration: 13.03 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.17, Duration: 10.79 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.17, Duration: 11.39 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.17, Duration: 12.20 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.18, Duration: 12.00 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.18, Duration: 13.22 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.18, Duration: 11.79 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.19, Duration: 12.60 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.19, Duration: 12.80 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.20, Duration: 14.01 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.20, Duration: 13.40 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.20, Duration: 13.81 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.20, Duration: 13.60 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.21, Duration: 14.82 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.21, Duration: 15.02 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.21, Duration: 14.21 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.22, Duration: 14.61 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.22, Duration: 15.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.22, Duration: 15.83 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.22, Duration: 16.23 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.23, Duration: 15.21 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.23, Duration: 16.02 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.23, Duration: 15.61 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.23, Duration: 16.83 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.23, Duration: 16.62 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.25, Duration: 17.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.25, Duration: 17.83 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.25, Duration: 17.01 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.26, Duration: 17.21 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.26, Duration: 14.36 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.26, Duration: 19.85 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.26, Duration: 16.39 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.26, Duration: 18.63 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.27, Duration: 17.61 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.27, Duration: 18.01 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.27, Duration: 19.23 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.28, Duration: 18.20 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.29, Duration: 20.44 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.29, Duration: 20.23 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.29, Duration: 19.01 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.29, Duration: 18.40 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.30, Duration: 18.80 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803116.30, Duration: 19.41 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n"
     ]
    },
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (503) from primary with message \"{\n  \"code\": 503,\n  \"type\": \"ServiceUnavailableException\",\n  \"message\": \"Model \\\"model\\\" has no worker to serve inference request. Please use scale workers API to add workers.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-2024-03-07-07-39-18-144 in account 699188727212 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# 결과 출력\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[0;32m---> 29\u001b[0m         start_time, duration, result \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest start time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Duration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, Result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 전체 요청 완료 후의 현재 시간\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m, in \u001b[0;36msend_request\u001b[0;34m(request_data)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_request\u001b[39m(request_data):\n\u001b[1;32m      8\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \n\u001b[0;32m----> 9\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \n\u001b[1;32m     15\u001b[0m     duration \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start  \n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (503) from primary with message \"{\n  \"code\": 503,\n  \"type\": \"ServiceUnavailableException\",\n  \"message\": \"Model \\\"model\\\" has no worker to serve inference request. Please use scale workers API to add workers.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-2024-03-07-07-39-18-144 in account 699188727212 for more information."
     ]
    }
   ],
   "source": [
    "# concurrent calls \n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# request function\n",
    "def send_request(request_data):\n",
    "    start = time.time()  \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,  \n",
    "        ContentType=content_type,\n",
    "        Body=json.dumps(request_data)\n",
    "    )\n",
    "    end = time.time()  \n",
    "    duration = end - start  \n",
    "    return start, duration, response['Body'].read().decode('utf-8')\n",
    "\n",
    "\n",
    "# 동시 요청의 수\n",
    "num_requests = 150\n",
    "\n",
    "\n",
    "# ThreadPoolExecutor를 사용하여 동시 요청 수행\n",
    "with ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "    futures = [executor.submit(send_request, data) for _ in range(num_requests)]\n",
    "\n",
    "    # 결과 출력\n",
    "    for future in futures:\n",
    "        start_time, duration, result = future.result()\n",
    "        print(f\"Request start time: {start_time:.2f}, Duration: {duration:.2f} seconds, Result: {result}\")\n",
    "\n",
    "# 전체 요청 완료 후의 현재 시간\n",
    "overall_end_time = time.time()\n",
    "\n",
    "# 시작 시간 중 가장 빠른 시간 찾기\n",
    "start_times = [future.result()[0] for future in futures]\n",
    "overall_start_time = min(start_times)\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "print(f\"Total duration for all requests: {overall_end_time - overall_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ea8da74-dac3-48a7-8eb0-eca2d347044b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request start time: 1709803060.77, Duration: 0.29 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.77, Duration: 0.50 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.77, Duration: 1.73 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.77, Duration: 4.20 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.77, Duration: 0.91 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.77, Duration: 4.40 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.78, Duration: 2.95 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.78, Duration: 0.70 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.78, Duration: 1.52 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.78, Duration: 3.98 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.78, Duration: 1.11 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.78, Duration: 2.13 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.79, Duration: 1.30 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.80, Duration: 2.73 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.80, Duration: 3.34 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.80, Duration: 1.91 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.81, Duration: 2.51 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.81, Duration: 3.74 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.82, Duration: 4.56 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.82, Duration: 3.53 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.82, Duration: 4.76 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.83, Duration: 2.29 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.83, Duration: 3.10 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.84, Duration: 4.95 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.84, Duration: 6.78 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.84, Duration: 5.15 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.84, Duration: 6.17 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.85, Duration: 5.34 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.86, Duration: 5.54 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.86, Duration: 7.78 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.86, Duration: 7.17 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.86, Duration: 6.35 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.87, Duration: 5.74 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.87, Duration: 8.61 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.87, Duration: 6.95 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.87, Duration: 5.93 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.88, Duration: 7.97 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.88, Duration: 7.35 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.88, Duration: 6.54 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.88, Duration: 9.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.88, Duration: 8.39 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.89, Duration: 7.54 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.90, Duration: 8.16 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.90, Duration: 8.99 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.90, Duration: 9.19 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.90, Duration: 9.80 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.91, Duration: 9.59 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.91, Duration: 10.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.91, Duration: 10.83 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.91, Duration: 10.21 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.92, Duration: 11.03 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.92, Duration: 8.77 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.93, Duration: 11.63 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.93, Duration: 9.98 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.94, Duration: 11.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.94, Duration: 10.60 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.94, Duration: 11.21 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.95, Duration: 12.23 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.96, Duration: 13.04 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.96, Duration: 12.01 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.96, Duration: 12.42 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.97, Duration: 12.62 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.97, Duration: 12.83 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.97, Duration: 14.88 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.97, Duration: 11.80 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.98, Duration: 13.22 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.98, Duration: 13.43 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.99, Duration: 14.45 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.99, Duration: 13.62 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.99, Duration: 14.04 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.99, Duration: 19.57 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803060.99, Duration: 13.83 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.00, Duration: 15.06 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.00, Duration: 15.26 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.01, Duration: 14.23 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.02, Duration: 15.85 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.02, Duration: 16.47 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.02, Duration: 15.64 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.02, Duration: 16.05 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.03, Duration: 15.44 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.03, Duration: 17.07 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.03, Duration: 17.48 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.04, Duration: 16.65 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.04, Duration: 18.08 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.05, Duration: 19.93 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.05, Duration: 18.49 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.05, Duration: 20.34 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.05, Duration: 16.23 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.05, Duration: 18.90 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.06, Duration: 17.25 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.06, Duration: 16.84 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.06, Duration: 19.71 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.06, Duration: 14.58 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.08, Duration: 19.28 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.08, Duration: 17.84 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.08, Duration: 18.25 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.08, Duration: 17.63 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.09, Duration: 20.10 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.09, Duration: 19.07 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Request start time: 1709803061.09, Duration: 18.65 seconds, Result: {\"generated_text\": [\"i had symptom\"]}\n",
      "Total duration for all requests: 20.63 seconds\n"
     ]
    }
   ],
   "source": [
    "# concurrent calls \n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# request function\n",
    "def send_request(request_data):\n",
    "    start = time.time()  \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,  \n",
    "        ContentType=content_type,\n",
    "        Body=json.dumps(request_data)\n",
    "    )\n",
    "    end = time.time()  \n",
    "    duration = end - start  \n",
    "    return start, duration, response['Body'].read().decode('utf-8')\n",
    "\n",
    "\n",
    "# 동시 요청의 수\n",
    "num_requests = 100\n",
    "\n",
    "\n",
    "# ThreadPoolExecutor를 사용하여 동시 요청 수행\n",
    "with ThreadPoolExecutor(max_workers=num_requests) as executor:\n",
    "    futures = [executor.submit(send_request, data) for _ in range(num_requests)]\n",
    "\n",
    "    # 결과 출력\n",
    "    for future in futures:\n",
    "        start_time, duration, result = future.result()\n",
    "        print(f\"Request start time: {start_time:.2f}, Duration: {duration:.2f} seconds, Result: {result}\")\n",
    "\n",
    "# 전체 요청 완료 후의 현재 시간\n",
    "overall_end_time = time.time()\n",
    "\n",
    "# 시작 시간 중 가장 빠른 시간 찾기\n",
    "start_times = [future.result()[0] for future in futures]\n",
    "overall_start_time = min(start_times)\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "print(f\"Total duration for all requests: {overall_end_time - overall_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2c220-7674-47f2-8329-c4596c41937f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oneline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
